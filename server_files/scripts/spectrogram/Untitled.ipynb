{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "34659936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 16800)\n",
      "(16800, 64)\n",
      "(16800, 64)\n",
      "(512, 1)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorboardX import SummaryWriter\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import pdb\n",
    "import scipy.stats as stats\n",
    "from scipy.signal import spectrogram\n",
    "import sys\n",
    "import yaml\n",
    "import imp\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "# from data_loader import lumpDataset, ToTensor\n",
    "# from utils import *\n",
    "\n",
    "from models import *\n",
    "\n",
    "os.chdir('/raid/togzhan_syrymova/lump_project/scripts/spectrogram/')\n",
    "\n",
    "def reshape_data(x):\n",
    "    x = x.values[:]\n",
    "    print(np.shape(x))\n",
    "    x = x.reshape(x.shape[0], 15, x.shape[1]//15)# 15, x.shape[1]//15\n",
    "    x = np.swapaxes(x, 1, 2)\n",
    "    return x\n",
    "    \n",
    "def get_labels(x, count):\n",
    "    labels = x.loc[x.index.repeat(count)].reset_index(drop=True)\n",
    "    return labels.values[:]\n",
    "\n",
    "def extract_slices(x, window_len, stride_len):\n",
    "    width = x.shape[1]\n",
    "    x_sliced = x[:, 0:window_len, :]\n",
    "    count = 1\n",
    "    index_end = 0\n",
    "    while  index_end+stride_len <= width:\n",
    "        # extract input sequences\n",
    "        index_start = count*stride_len\n",
    "        index_end = count*stride_len + window_len\n",
    "        x_temp = x[:, index_start:index_end, :]\n",
    "        x_sliced = np.vstack((x_sliced, x_temp))\n",
    "        count += 1   \n",
    "    print('>> stride count: ', count)\n",
    "    return x_sliced, count\n",
    "\n",
    "def to_tensor(x, y, idx):\n",
    "    x, y = shuffle(x,y)\n",
    "    if idx == 1:\n",
    "        x = torch.FloatTensor(x.astype('float64'))\n",
    "        y = torch.FloatTensor(y.astype('float64'))\n",
    "    return x, y\n",
    "\n",
    "def compute_spectogram(x):\n",
    "    fs = 160\n",
    "    f, t, data = spectrogram(x[:, :, :], fs, axis = 1)\n",
    "    return f, t, np.swapaxes(data, 2, 3)\n",
    "\n",
    "def read_data(window_len, stride_len, path, idx, var):\n",
    "    # go to dir and read data \n",
    "    os.chdir(path)\n",
    "    csv_files_train = glob.glob('df_train' + var + '.csv') # *raw*8\n",
    "    csv_files_test = glob.glob('test*' + var + '.csv')\n",
    "    csv_files_dev = glob.glob('df_dev' + var + '.csv')\n",
    "    # train data\n",
    "    df_train_ = pd.read_csv(csv_files_train[0], sep = ',', header = None)\n",
    "    df_train_ = pd.DataFrame(df_train_.values[1:,:])\n",
    "    # test data\n",
    "    df_test1 = pd.read_csv(csv_files_test[0], sep = ',', header = None)\n",
    "    df_test2 = pd.read_csv(csv_files_test[1], sep = ',', header = None)\n",
    "    df_test_ = pd.concat([df_test1, df_test2], axis = 1)\n",
    "    df_test_ = df_test_.transpose()\n",
    "    # dev data\n",
    "    df_dev_ = pd.read_csv(csv_files_dev[0], sep = ',', header = None)\n",
    "    df_dev_ = pd.DataFrame(df_dev_.values[1:,:])\n",
    "    ################################################################\n",
    "    # Get labels \n",
    "    if var =='raw8':\n",
    "        train_y = np.floor(pd.DataFrame((df_train_.values[:,-1].astype('float64')))/10)\n",
    "        dev_y = np.floor(pd.DataFrame((df_dev_.values[:,-1].astype('float64')))/10)\n",
    "    else:\n",
    "        train_y = pd.DataFrame((df_train_.values[:,-1].astype('float64')))\n",
    "        dev_y = pd.DataFrame((df_dev_.values[:,-1].astype('float64')))\n",
    "    # Create test labels\n",
    "    test_y = pd.concat([ pd.DataFrame([0 for i in range(np.shape(df_test1)[1])]), pd.DataFrame([1 for i in range(np.shape(df_test2)[1])])], axis=0)\n",
    "    test_y = test_y.reset_index(drop=True)\n",
    "    ################################################################\n",
    "    # get data\n",
    "    df_train = pd.DataFrame(df_train_.values[:,:-1].astype('float64'))\n",
    "    df_dev = pd.DataFrame(df_dev_.values[:,:-1].astype('float64'))\n",
    "    df_test = df_test_.astype('float64')\n",
    "    ################################################################\n",
    "    # reshape to 3d \n",
    "    df_train = reshape_data(df_train)\n",
    "    df_dev = reshape_data(df_dev)\n",
    "    df_test = reshape_data(df_test)\n",
    "    ################################################################\n",
    "    # slice the data\n",
    "    df_train_sliced, count = extract_slices(df_train,  window_len, stride_len)\n",
    "    df_dev_sliced, count = extract_slices(df_dev,  window_len, stride_len)\n",
    "    df_test_sliced, count = extract_slices(df_test,  window_len, stride_len)\n",
    "    ################################################################\n",
    "    # compute z-score\n",
    "    df_train_sliced = stats.zscore(df_train_sliced, axis = 1)\n",
    "    df_dev_sliced = stats.zscore(df_dev_sliced, axis = 1)\n",
    "    df_test_sliced = stats.zscore(df_test_sliced, axis = 1)\n",
    "    plt.plot(df_train_sliced[1,:,3])\n",
    "    plt.show()\n",
    "    ################################################################\n",
    "    # adapt labeling to liced data \n",
    "    train_y_sliced = get_labels(train_y,  count)\n",
    "    dev_y_sliced = get_labels(dev_y,  count)\n",
    "    test_y_sliced = get_labels(test_y,  count)\n",
    "    ################################################################\n",
    "    # convert to tensor\n",
    "#     df_train_s, train_y_s = to_tensor(df_train_sliced, train_y_sliced, idx)\n",
    "#     df_dev_s, dev_y_s = to_tensor(df_dev_sliced, dev_y_sliced, idx)\n",
    "#     df_test_s, test_y_s = to_tensor(df_test_sliced, test_y_sliced, idx)\n",
    "    ################################################################\n",
    "    df_train_s, train_y_s = shuffle(df_train_sliced, train_y_sliced)\n",
    "    df_dev_s, dev_y_s = shuffle(df_dev_sliced, dev_y_sliced)\n",
    "    df_test_s, test_y_s = shuffle(df_test_sliced, test_y_sliced)\n",
    "    \n",
    "    return  df_train_sliced, train_y_sliced, df_dev_sliced, dev_y_sliced, df_test_sliced, test_y_sliced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9711a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(train_x, train_y, dev_x, dev_y, model, model_save_path, criterion, optimizer, device):\n",
    "    best_epoch = 0\n",
    "    dev_acc_max=0\n",
    "    epoch_max=0\n",
    "    writer = SummaryWriter(comment='__' + 'Overtesting')\n",
    "    os.chdir('/raid/togzhan_syrymova/lump_project/scripts/spectrogram/')\n",
    "    config = yaml.safe_load(open(\"config.yaml\"))\n",
    "    num_epochs = config['num_epochs']\n",
    "    batch_size = config['batch_size']\n",
    "    for e in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(0, train_x.shape[0], batch_size):\n",
    "            if i+batch_size >= train_x.shape[0]:\n",
    "                x = train_x[i:]#.float()\n",
    "                y = train_y[i:]#.float()\n",
    "\n",
    "            else:\n",
    "                x = train_x[i:i+batch_size]#.float()\n",
    "                y = train_y[i:i+batch_size]#.float()\n",
    "\n",
    "            y_pred = model(x.to(device))\n",
    "            loss = criterion(y_pred, y.to(device))\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        ################    \n",
    "        print(\"Epoch {}/{}, Train Loss: {:.3f}\".format(e+1, num_epochs, total_loss))\n",
    "        writer.add_scalar('total_loss: ', total_loss)\n",
    "        with open(model_save_path + '_total_loss.txt', \"a\") as myfile:\n",
    "            myfile.write(str(total_loss))\n",
    "            myfile.write(\"\\n\")\n",
    "        ################\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            output = model(dev_x.to(device))\n",
    "            output = (output>0.5).float()\n",
    "            acc    = accuracy_score(dev_y, output.cpu())\n",
    "            if acc > dev_acc_max:\n",
    "                dev_acc_max=acc\n",
    "                epoch_max=e+1\n",
    "                torch.save(model, model_save_path + '.pt')\n",
    "        print(\"Dev Accuracy: {:.3f}\".format(acc))\n",
    "        writer.add_scalar('Dev Accuracy: ', acc)\n",
    "        model.train()\n",
    "    print(\"##############################################\")\n",
    "    print(\"Best dev accuracy is {:.3f} at epoch {}\".format(dev_acc_max, epoch_max))\n",
    "    print(\"Number of class 1 samples: \", (dev_y>0.5).float().sum().item())\n",
    "    print(\"##############################################\")    \n",
    "    \n",
    "def check_nan(x):\n",
    "    if(np.isnan(x).any()):\n",
    "        print(\"contain NaN values\")\n",
    "    else:\n",
    "        print(\"does not contain NaN values\")\n",
    "        \n",
    "def test_model(model_path, device, test_x, test_y):\n",
    "    with torch.no_grad():\n",
    "    #             pdb.set_trace()\n",
    "        model = torch.load(model_path)\n",
    "        model.eval().to(device)\n",
    "        output = model(test_x.to(device))\n",
    "        output = (output>0.5).float()\n",
    "        acc    = accuracy_score(test_y, output.cpu())\n",
    "        print(\"Test Accuracy: {:.3f}\".format(acc))\n",
    "        print(model)\n",
    "\n",
    "################ \n",
    "####        ####\n",
    "################\n",
    "def main(model_type):\n",
    "    var = '_L_h1'#'_L_h1'\n",
    "    os.chdir('/raid/togzhan_syrymova/lump_project/scripts/spectrogram/')\n",
    "    config = yaml.safe_load(open(\"config.yaml\"))\n",
    "    print(config)\n",
    "    num_epochs = config['num_epochs']\n",
    "    batch_size = config['batch_size']\n",
    "    learning_rate = config['learning_rate']\n",
    "    channels_in = config['channels_in']\n",
    "    D_out = config['D_out']\n",
    "    H = config['hidden_size']\n",
    "    kernel_size = config['kernel_size']\n",
    "    stride_len = config['stride']\n",
    "    dropout = config['dropout'] \n",
    "    ##\n",
    "    window_len = config['window_len']\n",
    "    stride_len = config['stride_len']\n",
    "    path = config['path']\n",
    "    ################################################################\n",
    "    # GET DATA #\n",
    "    idx = 1\n",
    "    train_x_, train_y_, dev_x_, dev_y_, test_x_, test_y_ = read_data(window_len, stride_len, path, idx, var)\n",
    "    print('Dataset:', train_x_.shape, train_y_.shape, dev_x_.shape, dev_y_.shape, test_x_.shape, test_y_.shape)\n",
    "    ################################################################        \n",
    "    print('train_x ')        \n",
    "    check_nan(train_x_)  \n",
    "    print('dev_x ')        \n",
    "    check_nan(dev_x_)  \n",
    "    print('test_x ')        \n",
    "    check_nan(test_x_)   \n",
    "    ################################################################\n",
    "#     compute spectograms #\n",
    "#     f, t, train_x1 = compute_spectogram(train_x_)\n",
    "#     f, t, dev_x1 = compute_spectogram(dev_x_)\n",
    "#     f, t, test_x1 = compute_spectogram(test_x_)\n",
    "    idx = 1\n",
    "    train_x, train_y  = to_tensor(train_x1, train_y_, idx)\n",
    "#     l = np.shape(train_x)\n",
    "#     train_x = np.reshape(train_x, [l[0], l[1]*l[2], l[3]])\n",
    "    dev_x, dev_y = to_tensor(dev_x1, dev_y_, idx)\n",
    "#     l = np.shape(dev_x)\n",
    "#     dev_x = np.reshape(dev_x, [l[0], l[1]*l[2], l[3]])\n",
    "    test_x, test_y = to_tensor(test_x1, test_y_, idx)\n",
    "#     l = np.shape(test_x)\n",
    "#     test_x = np.reshape(test_x, [l[0], l[1]*l[2], l[3]])\n",
    "    train_x = np.swapaxes(train_x, 1, 2)\n",
    "    dev_x = np.swapaxes(dev_x, 1, 2)\n",
    "    test_x = np.swapaxes(test_x, 1, 2)\n",
    "    ###############################################################\n",
    "    train_x, train_y  = to_tensor(train_x_, train_y_, idx)\n",
    "    dev_x, dev_y  = to_tensor(dev_x_, dev_y_, idx)\n",
    "    test_x, test_y  = to_tensor(test_x_, test_y_, idx)\n",
    "    D_in = np.shape(train_x)[2]\n",
    "    print('Started training')\n",
    "    seed = 777\n",
    "    torch.manual_seed(seed)\n",
    "    gpu_id = 1\n",
    "    device = torch.device(\"cuda:\" + str(gpu_id))\n",
    "    ################################################################\n",
    "    # ADAPT DIRECTORY AND MODEL NAMING\n",
    "    # ADD MODEL TO README OF THE FOLDER\n",
    "    directory = '/raid/togzhan_syrymova/lump_project/models/'\n",
    "    os.chdir(directory)\n",
    "    # DEFIEN THE MODEL\n",
    "#     model = ConvLayerNet(batch_size, device, channels_in, D_out, H).to(device)\n",
    "    model = ConvLayerNet3(batch_size, device, channels_in, D_in, H).to(device)\n",
    "    model.to(device)\n",
    "    model_name = type(model).__name__\n",
    "    ################################################################\n",
    "    pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "    print('# of params: ', pytorch_total_params)\n",
    "    ################################################################  \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    ################################################################  \n",
    "    # CREATE DIR TO SAVE MODEL AND OTHEHR STUFF\n",
    "    if os.path.isdir(model_name)==False:\n",
    "        os.mkdir(model_name)\n",
    "    opt = str(optimizer).split(\"(\")[0]\n",
    "    model_name2 = model_name + '_epochs_' + str(num_epochs) + '_lr_' + str(learning_rate) + \"_bs_\" + str(batch_size) \\\n",
    "    + '_window_len_' + str(window_len) + '_stride_len_' + str(stride_len) + '_'+ str(opt).split(\" \")[0] + '_' + str(criterion).split(\"(\")[0]\n",
    "    \n",
    "    model_save_path = directory + model_name + '/' + model_name2\n",
    "    ################################################################  \n",
    "    # SAVE THE MODEL AND README\n",
    "    with open(directory + model_name + '/' + 'README_' + model_name2 + '.txt', \"a\") as myfile:\n",
    "            myfile.write(str(model))\n",
    "            myfile.write(\"\\n OPTIMIZE: \")\n",
    "            myfile.write(str(optimizer))\n",
    "            myfile.write(\"\\n CRITERION: \")\n",
    "            myfile.write(str(criterion))\n",
    "            myfile.write(\"\\n\")\n",
    "            [myfile.write(str(items) + '\\n') for items in str(config).split(\", \")]\n",
    "            myfile.write(\"\\n\")\n",
    "            \n",
    "    print(model_name)\n",
    "    print(model_save_path)\n",
    "    print(model)\n",
    "    train_model(train_x, train_y, dev_x, dev_y, model, model_save_path, criterion, optimizer, device)\n",
    "    print('Start testing')\n",
    "    test_model(model_save_path, device, test_x, test_y)\n",
    "    \n",
    "    exit()\n",
    "################\n",
    "####        ####\n",
    "################\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     model_type = 'Conv'\n",
    "#     main(model_type)\n",
    "    \n",
    "# os.chdir('/raid/togzhan_syrymova/lump_project/scripts/spectrogram/')\n",
    "# config = yaml.safe_load(open(\"config.yaml\"))\n",
    "# num_epochs = config['num_epochs']\n",
    "# learning_rate = config['learning_rate']\n",
    "# batch_size = config['batch_size']\n",
    "# valid_period = config['valid_period']\n",
    "# window_len = config['window_len']\n",
    "# stride_len = config['stride_len']\n",
    "# path = config['path']\n",
    "\n",
    "# x_train, y_train, x_dev, y_dev, x_test, y_test = read_data(window_len, stride_len, path)\n",
    "# print('Dataset:', x_train.shape, y_train.shape, x_dev.shape, y_dev.shape, x_test.shape, y_test.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
